# Copyright 2023 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import gc
import hashlib
import io
import os
import signal
import time
from copy import deepcopy
from pathlib import Path
from typing import List, Optional, Union

import numpy as np
import pandas as pd
import torch
from mlperf_common.frameworks.pyt import PyTCommunicationHandler
from mlperf_common.logging import MLLoggerWrapper
from mlperf_logging import mllog

import openfold.distributed as dist
import openfold.dynamic_axial_parallelism as dap
import openfold.model.inductor as inductor
import apex.contrib.openfold_triton.mha as mha
import openfold.power_measurement as pm
from openfold.checkpoint_utils import (
    map_init_state_dicts,
    save_last_checkpoint,
    save_val_checkpoint,
)
from openfold.config import AlphaFoldConfig
from openfold.cudagraph_wrapper import CudaGraphFunctionWrapper, CudaGraphModuleWrapper
from openfold.dataloaders import (
    InitialTrainingDataloaderPQ,
    InitialTrainingDataloaderPT,
    ValidationDataloader,
)
from openfold.datasets import InitialTrainingDataset, ValidationDataset
from apex.contrib.openfold_triton.fused_adam_swa import FusedAdamSWA
from openfold.gradient_clipping import AsyncGradientClipping, update_norm_from_buckets
from openfold.helpers import get_seed_from_string, get_timestamp_string, map_dict_values
from openfold.log_utils import save_logs
from openfold.loss import AlphaFoldLoss
from openfold.lr_scheduler import OpenFoldBenchmarkLRScheduler
from openfold.model.alphafold import AlphaFold
from openfold.model.triton import load_triton_auto_tuned_cache
from openfold.numpy_utils import NUMPY_SEED_MODULUS
from openfold.samplers import InitialTrainingSampler, ValidationSampler
from openfold.swa import AlphaFoldSWA
from openfold.torch_utils import (
    disable_tf32,
    enable_tf32,
    get_input_dtype_cast_fn,
    map_tensor_tree,
)
from openfold.validation_metrics import compute_validation_metrics
from openfold.warmup import run_training_warmup


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--training_dirpath",
        type=Path,
        required=True,
        help="Path to training output directory.",
    )
    parser.add_argument(
        "--pdb_mmcif_chains_filepath",
        type=Path,
        required=True,
        help="Path to mmcif chains CSV file generated by data preprocessing.",
    )
    parser.add_argument(
        "--pdb_mmcif_dicts_dirpath",
        type=Path,
        required=True,
        help="Path to mmcif dicts directory generated by data preprocessing.",
    )
    parser.add_argument(
        "--pdb_obsolete_filepath",
        type=Path,
        required=True,
        help="Path to `obsolete.dat` file.",
    )
    parser.add_argument(
        "--pdb_alignments_dirpath",
        type=Path,
        required=True,
        help="Path to PDB alignments directory generated by data preprocessing.",
    )
    parser.add_argument(
        "--train_max_pdb_release_date",
        type=str,
        default="2021-09-16",
        help="Max PDB release date for training.",
    )
    parser.add_argument(
        "--val_min_cameo_submission_date",
        type=str,
        default="2021-09-17",
        help="Min submission date for CAMEO validation.",
    )
    parser.add_argument(
        "--val_max_cameo_submission_date",
        type=str,
        default="2021-12-11",
        help="Max submission date for CAMEO validation.",
    )
    parser.add_argument(
        "--val_max_sequence_length",
        type=int,
        default=700,
        help="Max sequence length for filtering CAMEO validation set.",
    )
    parser.add_argument(
        "--target_avg_lddt_ca_value",
        type=float,
        default=0.8,
        help="Target avg lDDT-Ca value required to stop training.",
    )
    parser.add_argument(
        "--initialize_parameters_from",
        type=Path,
        default=None,
        help="""Optional path to `.pt` checkpoint file
        used for parameter initialization.""",
    )
    parser.add_argument(
        "--precision",
        choices=["fp32", "tf32", "bf16", "fp16", "amp"],
        default="tf32",
        help="Numerical precision.",
    )
    parser.add_argument(
        "--seed",
        type=str,
        default="1234567890",
        help="Global seed for pseudorandom number generators.",
    )
    parser.add_argument(
        "--num_train_iters",
        type=int,
        default=2000,
        help="Number of training iterations.",
    )
    parser.add_argument(
        "--log_every_iters",
        type=int,
        default=-1,
        help="""Save logs every given iteration.
        A non-positive value disables the log saving.""",
    )
    parser.add_argument(
        "--checkpoint_every_iters",
        type=int,
        default=0,
        help="""Save checkpoints every given iteration.
        A non-positive value disables the checkpoint saving.""",
    )
    parser.add_argument(
        "--keep_last_checkpoints",
        type=int,
        default=0,
        help="How many last checkpoints to keep.",
    )
    parser.add_argument(
        "--val_every_iters",
        type=int,
        default=40,
        help="Compute validation every given iteration.",
    )
    parser.add_argument(
        "--keep_val_checkpoints",
        type=int,
        default=0,
        help="How many val checkpoints to keep.",
    )
    parser.add_argument(
        "--local_batch_size",
        type=int,
        default=1,
        help="Local batch size.",
    )
    parser.add_argument(
        "--dap_size",
        type=int,
        default=0,
        help="""Dynamic Axial Parallelism (DAP) size: 1, 2, 4, 8.
        Set 0 to disable DAP.""",
    )
    parser.add_argument(
        "--base_lr",
        type=float,
        default=1e-3,
        help="Base learning rate value.",
    )
    parser.add_argument(
        "--warmup_lr_init",
        type=float,
        default=1e-5,
        help="Warm-up initial learning rate value.",
    )
    parser.add_argument(
        "--warmup_lr_iters",
        type=int,
        default=0,
        help="Num iterations for learning rate warm-up.",
    )
    parser.add_argument(
        "--gradient_accumulation_iters",
        type=int,
        default=1,
        help="""Gradient accumulation iters.
        The default value of 1 means no accumulation.
        When set to > 1, other _iters and _length args must be scaled accordingly.""",
    )
    parser.add_argument(
        "--initial_training_dataloader_type",
        choices=["InitialTrainingDataloaderPT", "InitialTrainingDataloaderPQ"],
        default="InitialTrainingDataloaderPT",
        help="""Initial training dataloader type.
        InitialTrainingDataloaderPT - standard PyTorch DataLoader with deterministic
        sample order.
        InitialTrainingDataloaderPQ - custom dataloader with non-blocking priority queue
        based on PyTorch multiprocessing. Ensures higher throughput at the cost of
        non-deterministic sample order. This dataloader does not wait for time-consuming
        samples, which results in biased sample order where 'faster' samples may appear
        before 'slow' ones more frequently than in deterministic sample order.""",
    )
    parser.add_argument(
        "--num_train_dataloader_workers",
        type=int,
        default=14,
        help="Num workers (subprocesses) for each instance of training dataloader.",
    )
    parser.add_argument(
        "--num_val_dataloader_workers",
        type=int,
        default=2,
        help="Num workers (subprocesses) for each instance of validation dataloader.",
    )
    parser.add_argument(
        "--train_dataloader_prefetch_factor",
        type=int,
        default=4,
        help="""Prefetch factor for each instance of training dataloader.
        Larger values increase queue sizes holding prepared samples and batches.""",
    )
    parser.add_argument(
        "--train_dataloader_threading",
        action="store_true",
        help="""Whether to use threading in training dataloader.""",
    )
    parser.add_argument(
        "--num_async_val_ranks",
        type=int,
        default=0,
        help="Num async validation ranks taken from the end.",
    )
    parser.add_argument(
        "--filter_by_alignments",
        action="store_true",
        help="Whether to filter out mmcif chains with no alignments.",
    )
    parser.add_argument(
        "--use_only_pdb_chain_ids",
        type=str,
        nargs="*",
        default=None,
        help="""Optional list of pdb chain ids
        for intersection with train and val datasets.""",
    )
    parser.add_argument(
        "--save_process_logs",
        action="store_true",
        help="Whether to save logs from each process.",
    )
    parser.add_argument(
        "--mlperf_benchmark_type",
        choices=["TimeToTrain", "Throughput"],
        default="TimeToTrain",
        help="MLPerf benchmark type.",
    )
    parser.add_argument(
        "--mlperf_throughput_num_instances",
        type=int,
        default=os.environ.get("NUM_INSTANCES", 0),
        help="Num instances used for the MLPerf throughput-oriented benchmark.",
    )
    parser.add_argument(
        "--apilogs",
        action="store_true",
        help="Whether training script is used for apilogs generation.",
    )
    parser.add_argument(
        "--disable_warmup",
        action="store_true",
        help="Whether to disable training warmup before run_start.",
    )
    parser.add_argument(
        "--distributed",
        action="store_true",
        help="Whether to enable distributed training.",
    )
    args = parser.parse_args()
    # everything must be divisble by gradient accumulation length:
    assert args.gradient_accumulation_iters >= 1
    assert args.num_train_iters % args.gradient_accumulation_iters == 0
    assert args.val_every_iters % args.gradient_accumulation_iters == 0
    assert args.checkpoint_every_iters % args.gradient_accumulation_iters == 0
    assert args.log_every_iters % args.gradient_accumulation_iters == 0
    assert args.warmup_lr_iters % args.gradient_accumulation_iters == 0
    return args


def create_alphafold_module(
    alphafold_config: AlphaFoldConfig,
    device: torch.device,
    seed: int,
) -> AlphaFold:
    numpy_random_state = np.random.get_state()
    torch_rng_state = torch.get_rng_state()
    torch_cuda_rng_state = torch.cuda.get_rng_state(device=device)
    np.random.seed(seed % NUMPY_SEED_MODULUS)
    torch.manual_seed(seed)
    alphafold = AlphaFold(config=alphafold_config)
    alphafold.to(device=device)
    torch.cuda.set_rng_state(torch_cuda_rng_state, device=device)
    torch.set_rng_state(torch_rng_state)
    np.random.set_state(numpy_random_state)
    return alphafold


def initialize_parameters_from_checkpoint(
    alphafold: AlphaFold,
    optimizer: torch.optim.Optimizer,
    checkpoint_filepath: Path,
    device: torch.device,
    verbose: bool,
) -> str:
    init_checkpoint_bytes = checkpoint_filepath.read_bytes()
    init_checkpoint_sha256sum = hashlib.sha256(init_checkpoint_bytes).hexdigest()

    init_checkpoint = torch.load(io.BytesIO(init_checkpoint_bytes), map_location=device)
    is_resumable_checkpoint = bool(
        "alphafold_state_dict" in init_checkpoint
        and "optimizer_state_dict" in init_checkpoint
    )
    if is_resumable_checkpoint:
        init_alphafold_state_dict = init_checkpoint["alphafold_state_dict"]
        init_optimizer_state_dict = init_checkpoint["optimizer_state_dict"]
    else:
        init_alphafold_state_dict = init_checkpoint
        init_optimizer_state_dict = None

    new_init_state_dicts = map_init_state_dicts(
        alphafold_state_dict_keys=list(alphafold.state_dict().keys()),
        init_alphafold_state_dict=init_alphafold_state_dict,
        init_optimizer_state_dict=init_optimizer_state_dict,
    )
    init_alphafold_state_dict = new_init_state_dicts[0]
    init_optimizer_state_dict = new_init_state_dicts[1]

    # Initialize alphafold module:
    if verbose:
        print(f"Initializing parameters from {repr(checkpoint_filepath)}...")
    alphafold.load_state_dict(init_alphafold_state_dict, strict=True)
    if verbose:
        print(f"Parameters initialized from {repr(checkpoint_filepath)} successfully!")

    # Initialize optimizer state:
    if init_optimizer_state_dict is not None:
        if verbose:
            print(f"Initializing optimizer from {repr(checkpoint_filepath)}...")
        optimizer.load_state_dict(init_optimizer_state_dict)
        if verbose:
            print(
                f"Optimizer initialized from {repr(checkpoint_filepath)} successfully!"
            )

    return init_checkpoint_sha256sum


def set_alphafold_chunk_size(
    alphafold: Union[AlphaFold, AlphaFoldSWA],
    chunk_size: Union[int, None],
) -> None:
    for module in alphafold.modules():
        if hasattr(module, "chunk_size"):
            module.chunk_size = chunk_size


def validation_loop(
    alphafold: Union[AlphaFold, AlphaFoldSWA],
    validation_dataloader: ValidationDataloader,
    device: torch.device,
) -> List[dict]:
    alphafold.eval()
    if not dist.is_async_val_enabled():
        set_alphafold_chunk_size(alphafold, 128)
    val_metrics_list = []
    val_batch_iterator = iter(validation_dataloader)
    for _ in range(len(validation_dataloader)):
        perf_val_iter = perf_next_batch = -time.perf_counter()
        val_batch = next(val_batch_iterator)
        perf_next_batch += time.perf_counter()
        assert len(val_batch["id"]) == 1
        id_tuple = val_batch["id"][0]
        with torch.no_grad():
            val_batch = map_tensor_tree(
                fn=lambda t: t.to(device=device),
                tree=val_batch,
            )
            val_outputs = alphafold(val_batch)
            val_batch = map_tensor_tree(fn=lambda t: t[..., -1], tree=val_batch)
            val_metrics = compute_validation_metrics(
                predicted_atom_positions=val_outputs["final_atom_positions"],
                target_atom_positions=val_batch["all_atom_positions"],
                atom_mask=val_batch["all_atom_mask"],
                metrics_names={"lddt_ca"},
            )
        val_metrics = map_dict_values(fn=lambda t: t.item(), d=val_metrics)
        perf_val_iter += time.perf_counter()
        val_metrics["val_index"] = id_tuple[1]
        if dist.is_initialized():
            val_metrics["val_rank"] = dist.rank()
        else:
            val_metrics["val_rank"] = 0
        val_metrics["perf.val_iter"] = perf_val_iter
        val_metrics["perf.next_batch"] = perf_next_batch
        val_metrics_list.append(val_metrics)
    if not dist.is_async_val_enabled():
        set_alphafold_chunk_size(alphafold, None)
    alphafold.train()
    return val_metrics_list


def validation(
    alphafold: Union[AlphaFold, AlphaFoldSWA],
    validation_dataloader: ValidationDataloader,
    validation_dataset: ValidationDataset,
    device: torch.device,
    iteration: int,
    checkpoints_dirpath: Path,
    keep_val_checkpoints: int,
    is_logging_enabled: bool,
    val_logs_outpath: Path,
    mllogger: MLLoggerWrapper,
    epoch_num: int,
    mlperf_instance: int,
    target_avg_lddt_ca_value: Optional[float] = None,
) -> bool:
    # Start MLPerf evaluation measurement:
    mllogger.start(
        key=mllogger.constants.EVAL_START,
        sync=False,
        metadata={"epoch_num": epoch_num, "instance": mlperf_instance},
        unique_log_rank=dist.main_val_rank(),
    )

    # Start validation perf measurement:
    perf_validation = -time.perf_counter()
    if dist.is_main_val_process() and is_logging_enabled:
        print("validation...")

    # Disable DAP for eval:
    dap.disable()

    # Disable inductor kernels for eval:
    inductor.disable()

    # Execute validation loop:
    val_metrics_list = validation_loop(
        alphafold=alphafold,
        validation_dataloader=validation_dataloader,
        device=device,
    )

    # Reenable inductor kernels after eval:
    inductor.enable()

    # Reenable DAP after eval:
    dap.enable()

    # Collect per-sample validation metrics to main val process:
    val_metrics_list = dist.gather_val_metrics(
        val_metrics_list=val_metrics_list,
        device=device,
        synchronize=True,
    )

    # Update val metrics with info from dataset:
    if dist.is_main_val_process():
        for val_metrics in val_metrics_list:
            val_index = val_metrics["val_index"]
            val_metrics.update(validation_dataset.mmcif_chains[val_index])

    # Finalize validation perf measurement:
    perf_validation += time.perf_counter()

    # Compute aggregated validation metrics in main val process:
    if dist.is_main_val_process():
        val_metrics_df = pd.DataFrame(val_metrics_list)
        val_avg_lddt_ca = float(val_metrics_df["lddt_ca"].mean())
        val_size = len(val_metrics_list)
        assert val_size == len(validation_dataset)
        val_throughput = val_size / perf_validation
        mllogger.event(
            key="eval_accuracy",
            value=val_avg_lddt_ca,
            metadata={"epoch_num": epoch_num, "instance": mlperf_instance},
            unique_log_rank=dist.main_val_rank(),
        )

    # Save validation logs:
    if dist.is_main_val_process() and is_logging_enabled:
        val_log = {
            "iteration": iteration,
            "avg_lddt_ca": val_avg_lddt_ca,
            "timestamp": get_timestamp_string(),
            "perf.validation": perf_validation,
            "size": val_size,
            "throughput": val_throughput,
        }
        print(f"validation {val_log}")
        val_log["metrics_list"] = val_metrics_list
        save_logs([val_log], val_logs_outpath, append=True)

    # Check if validation reaches target accuracy:
    log_run_stop = False
    stop_training_flag = torch.tensor([0], device=device)
    if target_avg_lddt_ca_value is not None:
        if dist.is_main_val_process():
            if val_avg_lddt_ca >= target_avg_lddt_ca_value:
                log_run_stop = True
                stop_training_flag = torch.tensor([1], device=device)
            # override if power measurement is enabled:
            if pm.is_enabled():
                if stop_training_flag:
                    log_run_stop = pm.mark_as_converged()
                stop_training_flag = torch.tensor(
                    [int(pm.is_completed(verbose=True))], device=device
                )
        if dist.is_initialized():
            torch.distributed.broadcast(
                tensor=stop_training_flag,
                src=dist.main_val_rank(),
                group=dist.val_process_group(),
            )

    # Save validation checkpoint:
    if dist.is_main_val_process():
        save_val_checkpoint(
            alphafold=alphafold,
            iteration=iteration,
            checkpoints_dirpath=checkpoints_dirpath,
            keep_val_checkpoints=keep_val_checkpoints,
            val_avg_lddt_ca=val_avg_lddt_ca,
        )

    # End MLPerf evaluation measurement:
    mllogger.end(
        key=mllogger.constants.EVAL_STOP,
        sync=False,
        metadata={"epoch_num": epoch_num, "instance": mlperf_instance},
        unique_log_rank=dist.main_val_rank(),
    )

    # End MLPerf time-to-train (TTT) measurement:
    if log_run_stop:
        mllogger.log_run_stop(
            status=mllogger.constants.SUCCESS,
            sync=False,
            unique_log_rank=dist.main_val_rank(),
        )

    return bool(stop_training_flag)


def send_to_async_validation(
    alphafold: Union[AlphaFold, AlphaFoldSWA],
    device: torch.device,
) -> bool:
    assert dist.is_async_val_enabled()

    stop_training_flag = torch.tensor([0], device=device)

    # Communicate with main val rank:
    if dist.is_main_train_process():
        alphafold = deepcopy(alphafold)
        alphafold.to(device=torch.device("cpu"))
        alphafold_parameters = alphafold.parameters()

        # Synchronize train-val bridge:
        torch.distributed.barrier(group=dist.train_async_val_comm_process_group())

        # Receive stop training flag from main val rank:
        stop_training_flag = stop_training_flag.to(device=torch.device("cpu"))
        torch.distributed.broadcast(
            tensor=stop_training_flag,
            src=dist.main_val_rank(),
            group=dist.train_async_val_comm_process_group(),
        )
        stop_training_flag = stop_training_flag.to(device=device)

        # Send alphafold parameters to main val rank:
        if not stop_training_flag:
            for param in alphafold_parameters:
                torch.distributed.broadcast(
                    tensor=param.data,
                    src=dist.main_train_rank(),
                    group=dist.train_async_val_comm_process_group(),
                )

    # Synchronize train ranks:
    torch.distributed.barrier(group=dist.train_process_group())

    # Broadcast stop training flag among train ranks:
    torch.distributed.broadcast(
        tensor=stop_training_flag,
        src=dist.main_train_rank(),
        group=dist.train_process_group(),
    )

    return bool(stop_training_flag)


def run_async_validation(
    alphafold: Union[AlphaFold, AlphaFoldSWA],
    validation_dataloader: ValidationDataloader,
    validation_dataset: ValidationDataset,
    device: torch.device,
    num_train_iters: int,
    val_every_iters: int,
    first_val_iter: int,
    is_first_val_warmup: bool,
    global_batch_size: int,
    checkpoints_dirpath: Path,
    keep_val_checkpoints: int,
    is_logging_enabled: bool,
    val_logs_outpath: Path,
    mllogger: MLLoggerWrapper,
    mlperf_instance: int,
    target_avg_lddt_ca_value: Optional[float] = None,
) -> None:
    assert dist.is_async_val_enabled()

    stop_training_flag = torch.tensor([0], device=device)

    for iteration in range(first_val_iter, num_train_iters + 1, val_every_iters):
        is_warmup_iter = is_first_val_warmup and iteration == first_val_iter
        train_val_cycle_i = (iteration - 1) // val_every_iters + 1
        train_val_cycle_size = global_batch_size * val_every_iters
        epoch_num = train_val_cycle_i * train_val_cycle_size
        alphafold.to(device=torch.device("cpu"))
        alphafold_parameters = alphafold.parameters()

        # Communicate with main train rank:
        if dist.is_main_val_process() and not is_warmup_iter:
            # Synchronize train-val bridge:
            torch.distributed.barrier(group=dist.train_async_val_comm_process_group())

            # Send stop training flag to main train rank:
            torch.distributed.broadcast(
                tensor=stop_training_flag,
                src=dist.main_val_rank(),
                group=dist.train_async_val_comm_process_group(),
            )

            # Receive alphafold parameters from main train rank:
            if not stop_training_flag:
                for param in alphafold_parameters:
                    torch.distributed.broadcast(
                        tensor=param.data,
                        src=dist.main_train_rank(),
                        group=dist.train_async_val_comm_process_group(),
                    )

        # Synchronize val ranks:
        torch.distributed.barrier(group=dist.val_process_group())
        if stop_training_flag:
            return
        alphafold.to(device=device)
        alphafold_parameters = alphafold.parameters()

        # Broadcast alphafold parameters among val ranks:
        torch.distributed.barrier(group=dist.val_process_group())
        for param in alphafold_parameters:
            torch.distributed.broadcast(
                tensor=param.data,
                src=dist.main_val_rank(),
                group=dist.val_process_group(),
            )

        # Run evaluation on val ranks:
        stop_training_flag = validation(
            alphafold=alphafold,
            validation_dataloader=validation_dataloader,
            validation_dataset=validation_dataset,
            device=device,
            iteration=iteration,
            checkpoints_dirpath=checkpoints_dirpath,
            keep_val_checkpoints=keep_val_checkpoints,
            is_logging_enabled=is_logging_enabled,
            val_logs_outpath=val_logs_outpath,
            mllogger=mllogger,
            epoch_num=epoch_num,
            mlperf_instance=mlperf_instance,
            target_avg_lddt_ca_value=target_avg_lddt_ca_value,
        )
        stop_training_flag = torch.tensor([int(stop_training_flag)], device=device)


def training(args: argparse.Namespace) -> None:
    if args.distributed:
        # Assuming distributed training:
        dist.initialize()
        process_name = f"dist_process_rank{dist.rank()}"
        device = torch.device(f"cuda:{dist.local_rank()}")
        if args.num_async_val_ranks > 0:
            dist.initialize_async_val(num_async_val_ranks=args.num_async_val_ranks)
        global_batch_size = args.local_batch_size * len(dist.train_ranks())
        if args.dap_size > 0:
            assert len(dist.train_ranks()) % args.dap_size == 0
            global_batch_size //= args.dap_size
            dap.initialize(dap_size=args.dap_size)
        if dist.is_main_process():
            print(
                "initialized distributed training: "
                f"WORLD_SIZE={dist.world_size()} "
                f"GLOBAL_BATCH_SIZE={global_batch_size} "
                f"DAP_SIZE={dap.size()} "
                f"NUM_TRAIN_RANKS={dist.num_train_ranks()} "
                f"NUM_VAL_RANKS={dist.num_val_ranks()} "
                f"TRAIN_RANKS={dist.train_ranks()} "
                f"VAL_RANKS={dist.val_ranks()} "
                f"ASYNC_VAL_ENABLED={dist.is_async_val_enabled()} "
            )
    else:
        # Assuming single GPU training:
        print("single GPU training")
        process_name = "single_process"
        device = torch.device("cuda:0")
        global_batch_size = args.local_batch_size
        assert args.dap_size == 0

    # Print args:
    if dist.is_main_process():
        print(args)

    # Set device:
    torch.cuda.set_device(device=device)

    # Distributed warm-up:
    if args.distributed:
        if dist.is_train_rank():
            torch.distributed.barrier(
                group=dist.train_process_group(),
                device_ids=[dist.local_rank()],
            )
            if dap.is_initialized():
                torch.distributed.barrier(
                    group=dap.group(),
                    device_ids=[dist.local_rank()],
                )
        if dist.is_async_val_enabled():
            if dist.is_val_rank():
                torch.distributed.barrier(
                    group=dist.val_process_group(),
                    device_ids=[dist.local_rank()],
                )
            if dist.is_main_train_process() or dist.is_main_val_process():
                torch.distributed.barrier(
                    group=dist.train_async_val_comm_process_group(),
                    device_ids=None,
                )

    # Create output directory:
    args.training_dirpath.mkdir(parents=True, exist_ok=True)
    checkpoints_dirpath = args.training_dirpath / "checkpoints"

    # MLPerf logging setup:
    if args.mlperf_benchmark_type == "TimeToTrain":
        mlperf_instance = 0
    elif args.mlperf_benchmark_type == "Throughput":
        assert (
            int(os.environ.get("SLURM_JOB_NUM_NODES", "1"))
            % args.mlperf_throughput_num_instances
            == 0
        )
        mllog_datestamp = os.environ.get("DATESTAMP", "yymmddHHMMSSfffffffff")
        mlperf_instance = int(os.environ.get("EXP_ID", "0"))
        mllog_filename = f"{mllog_datestamp}_{mlperf_instance}.log"
        mllog_filepath = args.training_dirpath / mllog_filename
        mllog.config(filename=str(mllog_filepath))
    else:
        raise ValueError(f"unknown {repr(args.mlperf_benchmark_type)}")
    mllogger = MLLoggerWrapper(PyTCommunicationHandler(), value=None)
    mllogger.start(key=mllogger.constants.INIT_START, sync=True)
    mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)
    mllogger.mlperf_submission_log(benchmark="openfold", num_nodes=dist.num_nodes())
    mllogger.event(key=mllogger.constants.SEED, value=args.seed)
    mllogger.event(key="number_of_ranks", value=dist.world_size())
    mllogger.event(key="number_of_nodes", value=dist.num_nodes())
    mllogger.event(key="accelerators_per_node", value=dist.local_world_size())
    mllogger.event(key=mllogger.constants.GLOBAL_BATCH_SIZE, value=global_batch_size)
    mllogger.event(
        key=mllogger.constants.GRADIENT_ACCUMULATION_STEPS,
        value=args.gradient_accumulation_iters,
    )
    mllogger.event(key="target_avg_lddt_ca_value", value=args.target_avg_lddt_ca_value)
    mllogger.event(
        key="train_max_pdb_release_date", value=args.train_max_pdb_release_date
    )
    mllogger.event(
        key="val_min_cameo_submission_date", value=args.val_min_cameo_submission_date
    )
    mllogger.event(
        key="val_max_cameo_submission_date", value=args.val_max_cameo_submission_date
    )
    mllogger.event(key="val_max_sequence_length", value=args.val_max_sequence_length)
    mllogger.event(key="val_every_iters", value=args.val_every_iters)

    # Numerical precision settings:
    if args.precision == "fp32":
        disable_tf32()
        input_dtype = torch.float32
    elif args.precision == "tf32":
        enable_tf32()
        input_dtype = torch.float32
    elif args.precision == "bf16":
        enable_tf32()
        input_dtype = torch.bfloat16
    elif args.precision in {"fp16", "amp"}:
        raise NotImplementedError(f"precision={repr(args.precision)}")
    else:
        raise ValueError(f"unknown precision={repr(args.precision)}")
    mllogger.event(key="precision", value=args.precision)

    # Function to cast input tensors to the input dtype:
    to_input_dtype = get_input_dtype_cast_fn(input_dtype)

    # Get alphafold config:
    alphafold_config = AlphaFoldConfig.from_preset(
        stage="initial_training",
        precision=args.precision,
    )

    # Enable inductor kernels:
    inductor.enable()

    # Update config for apilogs generation:
    if args.apilogs:
        alphafold_config.cuda_graphs = False

    # Print alphafold_config:
    if dist.is_main_process():
        print(alphafold_config)

    # Setup TorchDynamo cache size:
    torch._dynamo.config.cache_size_limit = alphafold_config.dynamo_cache_size

    # Create alphafold module:
    alphafold_fp32 = create_alphafold_module(
        alphafold_config=alphafold_config,
        device=device,
        seed=get_seed_from_string(f"alphafold_init_{args.seed}"),
    )
    alphafold_fp32.train()
    alphafold_parameters_fp32 = list(alphafold_fp32.parameters())
    load_triton_auto_tuned_cache(dap_size=dap.size(), verbose=dist.is_main_process())
    mllogger.event(
        key="train_sequence_crop_size", value=alphafold_config.train_sequence_crop_size
    )
    mllogger.event(
        key="num_recycling_iters", value=alphafold_config.num_recycling_iters
    )
    mllogger.event(key="max_msa_clusters", value=alphafold_config.max_msa_clusters)
    mllogger.event(key="max_extra_msa", value=alphafold_config.max_extra_msa)
    mllogger.event(key="templates_enabled", value=alphafold_config.templates_enabled)
    mllogger.event(key="max_templates", value=alphafold_config.max_templates)

    # Create alphafold loss module:
    alphafold_loss = AlphaFoldLoss(config=alphafold_config.loss_config)
    mllogger.event(key="fape_loss_weight", value=alphafold_loss.fape_loss_config.weight)
    mllogger.event(
        key="fape_loss_backbone_weight",
        value=alphafold_loss.fape_loss_config.backbone_weight,
    )
    mllogger.event(
        key="fape_loss_sidechain_weight",
        value=alphafold_loss.fape_loss_config.sidechain_weight,
    )
    mllogger.event(
        key="supervised_chi_loss_weight",
        value=alphafold_loss.supervised_chi_loss_config.weight,
    )
    mllogger.event(
        key="distogram_loss_weight", value=alphafold_loss.distogram_loss_config.weight
    )
    mllogger.event(
        key="masked_msa_loss_weight", value=alphafold_loss.masked_msa_loss_config.weight
    )
    mllogger.event(
        key="plddt_loss_weight", value=alphafold_loss.plddt_loss_config.weight
    )

    # Create optimizer:
    optimizer = torch.optim.Adam(
        params=alphafold_parameters_fp32,
        lr=args.base_lr,  # lr is controlled by lr_scheduler
        betas=(
            alphafold_config.optimizer_adam_beta_1,
            alphafold_config.optimizer_adam_beta_2,
        ),
        eps=alphafold_config.optimizer_adam_eps,
        weight_decay=alphafold_config.optimizer_adam_weight_decay,
        amsgrad=alphafold_config.optimizer_adam_amsgrad,
    )
    mllogger.event(key=mllogger.constants.OPT_NAME, value="Adam")
    mllogger.event(key=mllogger.constants.OPT_BASE_LR, value=args.base_lr)
    mllogger.event(
        key=mllogger.constants.OPT_ADAM_BETA_1,
        value=alphafold_config.optimizer_adam_beta_1,
    )
    mllogger.event(
        key=mllogger.constants.OPT_ADAM_BETA_2,
        value=alphafold_config.optimizer_adam_beta_2,
    )
    mllogger.event(
        key=mllogger.constants.OPT_ADAM_EPSILON,
        value=alphafold_config.optimizer_adam_eps,
    )
    mllogger.event(
        key=mllogger.constants.OPT_WEIGHT_DECAY,
        value=alphafold_config.optimizer_adam_weight_decay,
    )
    mllogger.event(key="opt_amsgrad", value=alphafold_config.optimizer_adam_amsgrad)
    mllogger.event(
        key="opt_gradient_clipping", value=alphafold_config.gradient_clipping
    )
    mllogger.event(
        key=mllogger.constants.OPT_GRADIENT_CLIP_NORM,
        value=alphafold_config.clip_grad_max_norm,
    )

    # Create learning rate scheduler:
    lr_scheduler = OpenFoldBenchmarkLRScheduler(
        base_lr=args.base_lr,
        warmup_lr_init=args.warmup_lr_init,
        warmup_lr_iters=args.warmup_lr_iters,
        optimizer=optimizer,
    )
    mllogger.event(key="opt_learning_rate_warmup_init", value=args.warmup_lr_init)
    mllogger.event(
        key=mllogger.constants.OPT_LR_WARMUP_STEPS, value=args.warmup_lr_iters
    )

    # Initialize parameters from checkpoint if provided:
    if args.initialize_parameters_from is not None:
        init_checkpoint_sha256sum = initialize_parameters_from_checkpoint(
            alphafold=alphafold_fp32,
            optimizer=optimizer,
            checkpoint_filepath=args.initialize_parameters_from,
            device=device,
            verbose=dist.is_main_process(),
        )
        mllogger.event(key="init_checkpoint_sha256sum", value=init_checkpoint_sha256sum)

    # Apply CudaGraphs on AlphaFold:
    if alphafold_config.cuda_graphs and dap.size() >= 4:
        alphafold_fp32._forward_iteration = CudaGraphFunctionWrapper(
            alphafold_fp32._forward_iteration,
            [alphafold_fp32],
        )
    elif alphafold_config.cuda_graphs and dap.size() >= 2:
        alphafold_fp32.evoformer_stack = CudaGraphModuleWrapper(
            alphafold_fp32.evoformer_stack,
        )
        alphafold_fp32.structure_module = CudaGraphModuleWrapper(
            alphafold_fp32.structure_module,
        )
    elif alphafold_config.cuda_graphs:
        alphafold_fp32.structure_module = CudaGraphModuleWrapper(
            alphafold_fp32.structure_module,
        )

    # Create optional SWA version of AlphaFold for evaluation and checkpoints:
    swa_alphafold = AlphaFoldSWA(
        alphafold=alphafold_fp32,
        enabled=alphafold_config.swa_enabled,
        decay_rate=alphafold_config.swa_decay_rate,
    )
    alphafold_parameters_swa = list(swa_alphafold.parameters())
    mllogger.event(key="swa_enabled", value=alphafold_config.swa_enabled)
    mllogger.event(key="swa_decay_rate", value=alphafold_config.swa_decay_rate)

    # Create low-precision copy of the alphafold module:
    if args.precision == "bf16":
        alphafold_bf16 = deepcopy(alphafold_fp32).to(dtype=torch.bfloat16)
        alphafold_bf16.structure_module.to(dtype=torch.float32)
        alphafold_bf16.auxiliary_heads.to(dtype=torch.float32)
        alphafold_parameters_bf16 = list(alphafold_bf16.parameters())

    # Replace vanilla Adam optimizer with FusedAdamSWA module:
    if alphafold_config.fused_adam_swa:
        assert args.precision == "bf16"
        assert alphafold_config.swa_enabled
        optimizer = FusedAdamSWA.from_optim(
            adam_optimizer=optimizer,
            fp32_params=alphafold_parameters_fp32,
            bf16_params=alphafold_parameters_bf16,
            swa_params=alphafold_parameters_swa,
            swa_decay_rate=alphafold_config.swa_decay_rate,
        )
        lr_scheduler.optimizer = optimizer

    # Choose MHA implementation:
    if alphafold_config.triton_mha:
        mha.enable()
    else:
        mha.disable()

    # Register DAP gradient scaling hooks:
    if dap.is_initialized():
        if args.precision == "bf16":
            alphafold_bf16.register_dap_gradient_scaling_hooks(dap_size=dap.size())
        else:
            alphafold_fp32.register_dap_gradient_scaling_hooks(dap_size=dap.size())

    # Distributed wrapper:
    if args.distributed and dist.is_train_rank():
        if alphafold_config.cuda_graphs:
            init_stream = torch.cuda.Stream()
            init_stream.wait_stream(torch.cuda.current_stream(device=device))
        else:
            init_stream = None
        with torch.cuda.stream(init_stream):
            if args.precision == "bf16":
                alphafold_bf16 = torch.nn.parallel.DistributedDataParallel(
                    module=alphafold_bf16,
                    process_group=dist.train_process_group(),
                )
            else:
                alphafold_fp32 = torch.nn.parallel.DistributedDataParallel(
                    module=alphafold_fp32,
                    process_group=dist.train_process_group(),
                )

    # Register asynchronous gradient clipping communication hooks:
    is_async_gradient_clipping = (
        args.distributed
        and dist.is_train_rank()
        and alphafold_config.fused_adam_swa
        and alphafold_config.gradient_clipping
    )
    if is_async_gradient_clipping:
        async_grad_clip_state = AsyncGradientClipping(
            device=device,
            comm_group=dist.train_process_group(),
        )
        if args.precision == "bf16":
            alphafold_bf16.register_comm_hook(
                state=async_grad_clip_state,
                hook=update_norm_from_buckets,
            )
        else:
            alphafold_fp32.register_comm_hook(
                state=async_grad_clip_state,
                hook=update_norm_from_buckets,
            )

    # Log number of model parameters:
    alphafold_parameters_fp32_count = sum(p.numel() for p in alphafold_parameters_fp32)
    if args.precision == "bf16":
        alphafold_parameters_bf16_count = sum(
            p.numel() for p in alphafold_parameters_bf16
        )
        assert alphafold_parameters_bf16_count == alphafold_parameters_fp32_count
    mllogger.event(
        key="model_parameters_count",
        value=alphafold_parameters_fp32_count,
    )

    # Create logging-related objects:
    train_logs = []
    process_logs = []
    logs_dirpath = args.training_dirpath / "logs"
    train_logs_outpath = logs_dirpath / "training.log"
    process_logs_outpath = logs_dirpath / (process_name + ".log")
    val_logs_outpath = logs_dirpath / "validation.log"
    is_logging_enabled = bool(args.log_every_iters > 0)

    # Start data staging:
    mllogger.event(key="staging_start")
    staging_perf = -time.perf_counter()

    # <data staging code here>

    # Finalize data staging:
    staging_perf += time.perf_counter()
    mllogger.event(
        key="staging_stop",
        sync=False,
        metadata={"staging_duration": staging_perf, "instance": mlperf_instance},
    )
    mllogger.event(
        key="tracked_stats",
        sync=False,
        value={"staging_duration": staging_perf},
        metadata={"step": 0, "instance": mlperf_instance},
    )

    # Initialize cudagraph wrapper stream:
    if alphafold_config.cuda_graphs:
        side_stream = torch.cuda.Stream()
        torch.cuda.set_stream(side_stream)
        torch.cuda.current_stream(device=device).wait_stream(
            torch.cuda.default_stream(device=device)
        )
    else:
        side_stream = None

    # The memory usage of DAP2 is close to 80GB.
    # Set PYTORCH_CUDA_ALLOC_CONF to avoid fragmentation.
    # Setting PYTORCH_CUDA_ALLOC_CONF on eval nodes results in perf regression.
    if dap.size() == 2 and dist.is_async_val_enabled() and dist.is_train_rank():
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
        if dist.is_main_train_process() and is_logging_enabled:
            print(
                "Set PYTORCH_CUDA_ALLOC_CONF to max_split_size_mb:128 on train ranks."
            )

    # Training loop warmup:
    if dist.is_train_rank() and not args.disable_warmup:
        run_training_warmup(
            args=args,
            device=device,
            alphafold_config=alphafold_config,
            alphafold_fp32=alphafold_fp32,
            alphafold_bf16=alphafold_bf16,
            alphafold_parameters_bf16=alphafold_parameters_bf16,
            alphafold_loss=alphafold_loss,
            optimizer=optimizer,
            side_stream=side_stream,
            to_input_dtype=to_input_dtype,
        )

    # Start MLPerf time-to-train (TTT) measurement:
    mllogger.log_init_stop_run_start()

    # If power measurement enabled then mark the start:
    if pm.is_enabled():
        pm.start()
        args.num_train_iters = 2000
        if dist.is_main_process():
            print("power measurement started")

    # Create training dataset:
    if dist.is_train_rank():
        initial_training_dataset = InitialTrainingDataset(
            pdb_mmcif_chains_filepath=args.pdb_mmcif_chains_filepath,
            pdb_mmcif_dicts_dirpath=args.pdb_mmcif_dicts_dirpath,
            pdb_obsolete_filepath=args.pdb_obsolete_filepath,
            pdb_alignments_dirpath=args.pdb_alignments_dirpath,
            max_pdb_release_date=args.train_max_pdb_release_date,
            alphafold_config=alphafold_config,
            filter_by_alignments=args.filter_by_alignments,
            use_only_pdb_chain_ids=args.use_only_pdb_chain_ids,
            name=f"initial_training_dataset_{process_name}",
        )
        mllogger.event(
            key=mllogger.constants.TRAIN_SAMPLES,
            value=len(initial_training_dataset),
            unique_log_rank=dist.main_train_rank(),
        )

    # Create validation dataset:
    if dist.is_val_rank():
        validation_dataset = ValidationDataset(
            pdb_mmcif_chains_filepath=args.pdb_mmcif_chains_filepath,
            pdb_mmcif_dicts_dirpath=args.pdb_mmcif_dicts_dirpath,
            pdb_obsolete_filepath=args.pdb_obsolete_filepath,
            pdb_alignments_dirpath=args.pdb_alignments_dirpath,
            min_cameo_submission_date=args.val_min_cameo_submission_date,
            max_cameo_submission_date=args.val_max_cameo_submission_date,
            max_sequence_length=args.val_max_sequence_length,
            alphafold_config=alphafold_config,
            filter_by_alignments=args.filter_by_alignments,
            use_only_pdb_chain_ids=args.use_only_pdb_chain_ids,
            name=f"validation_dataset_{process_name}",
        )
        mllogger.event(
            key=mllogger.constants.EVAL_SAMPLES,
            value=len(validation_dataset),
            unique_log_rank=dist.main_val_rank(),
        )

    # Create training sampler:
    if dist.is_train_rank():
        initial_training_sampler = InitialTrainingSampler(
            dataset=initial_training_dataset,
            local_batch_size=args.local_batch_size,
            global_batch_size=global_batch_size,
            num_train_iters=args.num_train_iters,
            num_prev_iters=0,
            seed=get_seed_from_string(f"initial_training_sampler_{args.seed}"),
            is_distributed=args.distributed,
            dap_size=dap.size(),
        )

    # Create validation sampler:
    if dist.is_val_rank():
        validation_sampler = ValidationSampler(
            dataset=validation_dataset,
            is_distributed=args.distributed,
        )

    # Create training dataloader:
    if dist.is_train_rank():
        if args.initial_training_dataloader_type == "InitialTrainingDataloaderPT":
            InitialTrainingDataloader = InitialTrainingDataloaderPT
        elif args.initial_training_dataloader_type == "InitialTrainingDataloaderPQ":
            InitialTrainingDataloader = InitialTrainingDataloaderPQ
        else:
            raise ValueError(
                "unknown initial_training_dataloader_type="
                f"{repr(args.initial_training_dataloader_type)}"
            )
        initial_training_dataloader = InitialTrainingDataloader(
            dataset=initial_training_dataset,
            sampler=initial_training_sampler,
            local_batch_size=args.local_batch_size,
            num_workers=args.num_train_dataloader_workers,
            prefetch_factor=args.train_dataloader_prefetch_factor,
            seed=get_seed_from_string(f"initial_training_dataloader_{args.seed}"),
            uniform_recycling_iters=list(
                range(0, alphafold_config.num_recycling_iters + 1)
            ),
            gradient_accumulation_iters=args.gradient_accumulation_iters,
            num_prev_iters=0,
            use_threading=args.train_dataloader_threading,
        )
        train_batch_iterator = iter(initial_training_dataloader)
        mllogger.event(
            key="initial_training_dataloader_type",
            value=args.initial_training_dataloader_type,
            unique_log_rank=dist.main_train_rank(),
        )

    # Create validation dataloader:
    if dist.is_val_rank():
        validation_dataloader = ValidationDataloader(
            dataset=validation_dataset,
            sampler=validation_sampler,
            num_workers=args.num_val_dataloader_workers,
            use_cache=True,
        )

    # Select alphafold for evaluation:
    val_alphafold = swa_alphafold if swa_alphafold.enabled else alphafold_fp32

    # Disable garbage collector to remove anomalous performance drops:
    gc.disable()

    # Launch async evaluation:
    if dist.is_async_val_enabled() and dist.is_val_rank():
        if not args.disable_warmup and dap.size() > 2:
            # enable val warmup:
            first_val_iter = 0
            is_first_val_warmup = True
        else:
            # disable val warmup:
            first_val_iter = 0 + args.val_every_iters
            is_first_val_warmup = False
        run_async_validation(
            alphafold=val_alphafold,
            validation_dataloader=validation_dataloader,
            validation_dataset=validation_dataset,
            device=device,
            num_train_iters=args.num_train_iters,
            val_every_iters=args.val_every_iters,
            first_val_iter=first_val_iter,
            is_first_val_warmup=is_first_val_warmup,
            global_batch_size=global_batch_size,
            checkpoints_dirpath=checkpoints_dirpath,
            keep_val_checkpoints=args.keep_val_checkpoints,
            is_logging_enabled=is_logging_enabled,
            val_logs_outpath=val_logs_outpath,
            mllogger=mllogger,
            mlperf_instance=mlperf_instance,
            target_avg_lddt_ca_value=args.target_avg_lddt_ca_value,
        )
        torch.distributed.barrier()
        if dap.size() >= 2:
            torch.distributed.barrier()
            # TODO: When DAP>=2 Python doesn't exit after logging final 'run_stop'.
            #       Will investigate the root cause and remove the following code.
            print("os.killpg\n", end=None)
            os.killpg(os.getpgid(os.getpid()), signal.SIGKILL)
        return

    # Training loop:
    for iteration in range(1, args.num_train_iters + 1):
        # Train-val cycle:
        train_val_cycle_i = (iteration - 1) // args.val_every_iters + 1
        is_train_val_cycle_start = bool((iteration - 1) % args.val_every_iters == 0)
        is_train_val_cycle_end = bool(iteration % args.val_every_iters == 0)
        train_val_cycle_size = global_batch_size * args.val_every_iters
        epoch_num = train_val_cycle_i * train_val_cycle_size
        is_validation_iter = is_train_val_cycle_end

        # Reconfigure cuda stream:
        if alphafold_config.cuda_graphs and iteration == 20 and args.disable_warmup:
            torch.cuda.set_stream(torch.cuda.default_stream(device=device))
            torch.cuda.current_stream(device=device).wait_stream(side_stream)

        # Start MLPerf training throughput measurement:
        if is_train_val_cycle_start:
            mllogger.start(
                key=mllogger.constants.EPOCH_START,
                sync=False,
                metadata={"epoch_num": epoch_num, "instance": mlperf_instance},
            )
            perf_train_cycle = -time.perf_counter()
            perf_next_batch_list = []

        # Start training iteration perf measurement:
        perf_train_iter = -time.perf_counter()

        # Deterministic forward pass during training (dropout etc.):
        if dap.is_enabled():
            forward_seed_string = f"forward_{args.seed}_{dap.group_rank()}_{iteration}"
        elif args.distributed:
            forward_seed_string = f"forward_{args.seed}_{dist.rank()}_{iteration}"
        else:
            forward_seed_string = f"forward_{args.seed}_0_{iteration}"
        torch.manual_seed(get_seed_from_string(forward_seed_string))

        # Next train batch:
        perf_next_batch = -time.perf_counter()
        train_batch = next(train_batch_iterator)
        perf_next_batch += time.perf_counter()
        perf_next_batch_list.append(perf_next_batch)
        train_batch = map_tensor_tree(
            fn=lambda t: t.to(device=device),
            tree=train_batch,
        )
        num_recycling_iters = train_batch["aatype"].shape[-1] - 1

        # Forward pass:
        train_inputs = map_tensor_tree(fn=to_input_dtype, tree=train_batch)
        if args.precision == "bf16":
            train_outputs = alphafold_bf16(train_inputs)
        else:
            train_outputs = alphafold_fp32(train_inputs)
        loss, losses = alphafold_loss(
            outputs=train_outputs,
            batch=map_tensor_tree(fn=lambda t: t[..., -1], tree=train_batch),
        )
        loss = loss / args.gradient_accumulation_iters

        # Backward pass:
        if (iteration - 1) % args.gradient_accumulation_iters == 0:
            if args.precision == "bf16":
                for param_bf16 in alphafold_parameters_bf16:
                    param_bf16.grad = None
            else:
                optimizer.zero_grad()
        loss.backward()

        if iteration % args.gradient_accumulation_iters == 0:
            # Gradient clipping:
            if alphafold_config.gradient_clipping and not is_async_gradient_clipping:
                if args.precision == "bf16":
                    torch.nn.utils.clip_grad_norm_(
                        parameters=alphafold_parameters_bf16,
                        max_norm=alphafold_config.clip_grad_max_norm,
                    )
                else:
                    torch.nn.utils.clip_grad_norm_(
                        parameters=alphafold_parameters_fp32,
                        max_norm=alphafold_config.clip_grad_max_norm,
                    )

            # LR scheduler update:
            lr_scheduler(iteration)

            # Optimizer step:
            if alphafold_config.fused_adam_swa:
                # Fused optimizer step (casting BF16/FP32 + param updates + SWA):
                if is_async_gradient_clipping:
                    grad_clip_scale = async_grad_clip_state.get_clip_scale(
                        max_norm=alphafold_config.clip_grad_max_norm,
                    )
                    optimizer.step(grad_clip_scale=grad_clip_scale)
                else:
                    optimizer.step()
            else:
                # Vanilla optimizer step:
                if args.precision == "bf16":
                    # Assign bf16 gradient to fp32 params:
                    for param_fp32, param_bf16 in zip(
                        alphafold_parameters_fp32, alphafold_parameters_bf16
                    ):
                        param_fp32.grad = param_bf16.grad.to(dtype=torch.float32)
                # Optimizer step (weights/parameters update):
                optimizer.step()
                if args.precision == "bf16":
                    # Update bf16 params:
                    for param_fp32, param_bf16 in zip(
                        alphafold_parameters_fp32, alphafold_parameters_bf16
                    ):
                        param_bf16.data.copy_(param_fp32.data)
                # SWA update:
                if swa_alphafold.enabled:
                    swa_alphafold.update(alphafold_fp32)

        # Average losses from distributed training:
        if is_logging_enabled:
            losses_avg = dist.reduce_losses_avg(losses=losses, device=device)
            # Convert losses from Dict[str, torch.Tensor] to Dict[str, float]:
            losses = map_dict_values(fn=lambda t: t.item(), d=losses)
            if dist.is_main_train_process():
                losses_avg = map_dict_values(fn=lambda t: t.item(), d=losses_avg)

        # Finalize training iteration perf measurement:
        perf_train_iter += time.perf_counter()

        # Update process logs:
        if is_logging_enabled and args.save_process_logs:
            process_log = {
                "iteration": iteration,
                "sample_ids": list(map(list, train_batch["id"])),
                "num_recycling_iters": num_recycling_iters,
                "timestamp": get_timestamp_string(),
                **{f"losses.{k}": v for k, v in losses.items()},
                "perf.train_iter": perf_train_iter,
                "perf.next_batch": perf_next_batch,
                "batch.priority": train_batch.get("__priority__", -1),
            }
            process_logs.append(process_log)

        # Update train logs:
        if dist.is_main_train_process() and is_logging_enabled:
            train_log = {
                "iteration": iteration,
                "global_batch_size": global_batch_size,
                "num_recycling_iters": num_recycling_iters,
                "timestamp": get_timestamp_string(),
                **{f"losses_avg.{k}": v for k, v in losses_avg.items()},
                "perf.train_iter": perf_train_iter,
            }
            train_logs.append(train_log)
            print(f"training {train_log}")

        # Save process and train logs:
        if is_logging_enabled and iteration % args.log_every_iters == 0:
            if args.save_process_logs:
                save_logs(process_logs, process_logs_outpath, append=True)
            process_logs.clear()
            if dist.is_main_train_process():
                save_logs(train_logs, train_logs_outpath, append=True)
                train_logs.clear()

        # End MLPerf training throughput measurement:
        if is_train_val_cycle_end:
            mllogger.end(
                key=mllogger.constants.EPOCH_STOP,
                sync=False,
                metadata={"epoch_num": epoch_num, "instance": mlperf_instance},
            )
            perf_train_cycle += time.perf_counter()
            mllogger.event(
                key="tracked_stats",
                sync=False,
                value={"throughput": train_val_cycle_size / perf_train_cycle},
                metadata={"step": train_val_cycle_i, "instance": mlperf_instance},
            )
            avg_perf_next_batch = sum(perf_next_batch_list) / len(perf_next_batch_list)
            mllogger.event(
                key="tracked_stats",
                sync=False,
                value={"avg_perf_next_batch": avg_perf_next_batch},
                metadata={"step": train_val_cycle_i, "instance": mlperf_instance},
            )

        # Validation phase:
        if is_validation_iter:
            if dist.is_async_val_enabled():
                # Asynchronous evaluation on separate val ranks:
                stop_training_flag = send_to_async_validation(
                    alphafold=val_alphafold,
                    device=device,
                )
            else:
                del train_batch, train_inputs, train_outputs, loss
                # Synchronous evaluation on the same ranks as training:
                stop_training_flag = validation(
                    alphafold=val_alphafold,
                    validation_dataloader=validation_dataloader,
                    validation_dataset=validation_dataset,
                    device=device,
                    iteration=iteration,
                    checkpoints_dirpath=checkpoints_dirpath,
                    keep_val_checkpoints=args.keep_val_checkpoints,
                    is_logging_enabled=is_logging_enabled,
                    val_logs_outpath=val_logs_outpath,
                    mllogger=mllogger,
                    epoch_num=epoch_num,
                    mlperf_instance=mlperf_instance,
                    target_avg_lddt_ca_value=args.target_avg_lddt_ca_value,
                )

        # Save latest checkpoint:
        if (
            dist.is_main_train_process()
            and args.checkpoint_every_iters > 0
            and iteration % args.checkpoint_every_iters == 0
        ):
            save_last_checkpoint(
                alphafold=alphafold_fp32,
                optimizer=optimizer,
                swa_alphafold=swa_alphafold,
                iteration=iteration,
                checkpoints_dirpath=checkpoints_dirpath,
                keep_last_checkpoints=args.keep_last_checkpoints,
            )

        # Stop training if reached target validation metric:
        if is_validation_iter and stop_training_flag:
            break

    # Synchronize before return:
    if args.distributed:
        if dist.is_train_rank():
            del train_batch_iterator
            del initial_training_dataloader
        torch.distributed.barrier()

    # Exit training:
    if dap.size() >= 2:
        torch.distributed.barrier()
        # TODO: When DAP>=2 Python doesn't exit after logging final 'run_stop'.
        #       Will investigate the root cause and remove the following code.
        print("os.killpg\n", end=None)
        os.killpg(os.getpgid(os.getpid()), signal.SIGKILL)


if __name__ == "__main__":
    try:
        training(parse_args())
    except KeyboardInterrupt:
        print("KeyboardInterrupt... exit(1)")
        exit(1)
